# -*- coding: utf-8 -*-

'''
У вас есть много url-адресов, порядка 10 миллиардов.
Как бы вы организовали эффективный поиск дубликатов,
учитывая, что все они, конечно же, не поместятся
в памяти?
'''

# Скорее всего такое количество url-адресов будет храниться
# в некольких файлах. Так, что нам понадобится список
# с именами файлов.

# Необходимо будет задать размер рабочей памяти - это
# память с которой есть возможность работать.

# Затем из каждого файла будем считывать в два раза меньше
# объёма рабочей памяти и отфильтрованные адреса
# будут записываться в новые файлы.

# Фильтрация будет происходить хешированием
# благодаря классу Counter.

# Размер новых файлов, максимально, будет меньше рабочей памяти в два раза.
# Это нужно для того, чтобы потом можно было полностью отфильтровать
# содержимое двух файлов.

# Каждый новый файл будет сравниваться с другими файлами
# и таким образом будут отфильтрованы все дубликаты адресов.

# Исходные файлы меняться не будут.

from urls_filter import URLFilter
from settings import NAME_URL_FILES, COUNT_MEM

if __name__ == '__main__':
    URLFilter(arr_file=NAME_URL_FILES, count_mem=COUNT_MEM).go()

